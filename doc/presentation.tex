\documentclass[compress,aspectratio=169]{beamer} %aspectratio=169

\usetheme{Hamburg}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{lmodern}

\usepackage[english]{babel}
%\usepackage[ngerman]{babel}

\usepackage{eurosym}
\usepackage{listings}
\usepackage{lstautogobble}
\usepackage{microtype}
\usepackage{textcomp}
\usepackage{units}
\usepackage{color}
\usepackage{ulem}
\usepackage{tabularx}
\usepackage{tikz,pgfplots}
\usetikzlibrary{positioning,shapes}

\renewcommand\tabularxcolumn[1]{m{#1}}% for vertical centering text in X column

\definecolor{old-code}{gray}{0.60}

\lstset{
	basicstyle=\ttfamily\footnotesize,
	frame=single,
	numbers=left,
	language=Python,
	breaklines=true,
	breakatwhitespace=true,
	postbreak=\hbox{$\hookrightarrow$ },
	showstringspaces=false,
	autogobble=true,
	upquote=true,
	tabsize=4,
	captionpos=b,
	morekeywords={int8_t,uint8_t,int16_t,uint16_t,int32_t,uint32_t,int64_t,uint64_t,size_t,ssize_t,off_t,intptr_t,uintptr_t,mode_t}
}

\title{One Million Posts Corpus}
\subtitle{Seminar Deep Learning for Language and Speech}
\author{Jens Becker, Julius Plehn, Oliver Pola}
\institute{Language Technology Group\\Fachbereich Informatik\\Fakultät für Mathematik, Informatik und Naturwissenschaften\\Universität Hamburg}
\date{25.02.2020}

\titlegraphic{\raggedright\includegraphics[trim={0mm 0mm 1.8cm 0mm},clip,height=1.2cm]{img/logo}}

\begin{document}

\begin{frame}
	\titlepage
\end{frame}

\begin{frame}
	\frametitle{Agenda}

	\tableofcontents[hidesubsections]
\end{frame}


\section{Corpus}

\begin{frame}[fragile]
	\frametitle{Corpus}
	\begin{itemize}
		\item One Million Posts Corpus
		\item User posts from website of Austrian daily newspaper DER STANDARD
		\item Taken over 12 months 2015-2016 
		\item 1,000,000 unlabeled posts
		\item 11,773 labeled posts
		\item Available at \url{https://ofai.github.io/million-post-corpus/}
	\end{itemize}
	\hfill\tiny\cite{Schabus17, Schabus18}
\end{frame}

\begin{frame}[fragile]
	\frametitle{Categories}
	\centering
	\begin{tabular}{l r r r r r}
		& Labeled & \multicolumn{2}{c}{Does apply} & \multicolumn{2}{c}{We apply} \\
		\hline
		Sentiment Negative & 3599 & 1691 & 47\% \\
		Sentiment Neutral & 3599 & 1865 & 52\% \\
		Sentiment Positive & 3599 & 43 & 1\% \\
		Off Topic & 3599 & 580 & 16\% \\
		Inappropriate & 3599 & 303 & 8\%\\
		Discriminating & 3599 & 282 & 8\%\\
		Possibly Feedback & 6038 & 1301 & 22\% & 72 & 2\%\\
		Personal Stories & 9336 & 1625 & 17\% & 47 & 1\%\\
		Arguments Used & 3599 & 1022 & 28\%\\
	\end{tabular}\\
	{\hfill\tiny\cite{Schabus17}}
	\vspace{5mm}
	\begin{itemize}
		\item We use only posts, that are annotated as 0 or 1 for each category
	\end{itemize}
\end{frame}


\section{Embedding}

\begin{frame}[fragile]
	\frametitle{Word2Vec Embedding}
	\begin{itemize}
		\item Using Word2Vec embedding~\cite{word2vec}
		\item Applied by Gensim~\cite{gensim}
		\item Loading pretrained german model~\cite{depset.ai}
		\item Vocabulary size = 1,309,281
		\item Embedding dim = 300
		\item Padded sequence length = 80
	\end{itemize}
\end{frame}

\begin{frame}[fragile]
	\frametitle{Embedding Method}
	\centering\small
	\begin{tabular}{l l l}
		& Method 1 & Method 2 \\
		\hline
		Preprocessing & posts to lists of word indices & posts to vectors \\
		\hline
		Embedding matrix & feed matrix to training model & not needed, discard all \\
		& & unseen words \\
		\hline
		Training & repeat feeding lists of word indices & repeat feeding vectors \\
		\hline
		Memory usage (GPU) & high & lower \\
		\hline
		Delete embedding model & no & yes \\
		after preprocessing \\
		\hline
		Memory usage (CPU) & high & lower \\
		\hline
		Applicable to low-end & no & yes \\
		systems \\
	\end{tabular}\\
\end{frame}


\section{DeepLearning}

\begin{frame}[fragile]
	\frametitle{Model}
	\begin{figure}
		 \centering
		\includegraphics[trim={0cm 19.5cm 0cm 3cm},clip,page=2, width=0.9\textwidth]{img/model}
	\end{figure}
	\begin{itemize}
		\item Implemented using Tensorflow 2 and Keras
		\item Supervised using automatic learning rate adaption (\textit{ReduceLROnPlateau}) and \textit{EarlyStopping}
	\end{itemize}
\end{frame}

\begin{frame}[fragile]
	\frametitle{Training}
	\hspace*{-1cm}
	\includegraphics[trim={1.6cm 12.4cm 2cm 0.5cm},clip,width=7.8cm]{img/training_SentimentNegative_100}%
	\includegraphics[trim={1.4cm 12.4cm 2cm 0.5cm},clip,width=7.8cm]{img/training_All_22}
	\small
	\vspace{-5mm}
	\begin{itemize}
		\item Left two: Single-Model for Sentiment Negative before implementing Early Stopping
		\begin{itemize}
			\item Although validation loss increases early, accuracy (precision, recall) still improve
		\end{itemize}
		\item Right two: Multi-Model with Early Stopping
	\end{itemize}
\end{frame}

\section{Results}

\begin{frame}[fragile]
	\frametitle{Results Single-/Multi-Model}
	\centering\scriptsize
	\begin{tabular}{l r r r r r r r r}
		& True Pos & True Neg & False Pos & False Neg & Accuracy & Precision & Recall & $F_1$ \\
		\hline
		Sentiment Negative & 145 & 159 & 99 & 137 & 0.56 & 0.59 & 0.51 & 0.55 \\
		& 112 & 236 & 59 & 133 & 0.64 & 0.65 & 0.46 & 0.54 \\
		\hline
		Sentiment Neutral & 190 & 133 & 149 & 68 & 0.60 & 0.56 & 0.74 & 0.64 \\
		& 216 & 124 & 126 & 74 & 0.63 & 0.63 & 0.75 & 0.68 \\
		\hline
		Sentiment Positive & 0 & 533 & 0 & 7 & 0.99 & 0 & 0 & 0 \\
		& 0 & 535 & 0 & 5 & 0.99 & 0 & 0 & 0 \\
		\hline
		Off Topic & 0 & 452 & 0 & 88 & 0.84 & 0 & 0 & 0 \\
		& 11 & 423 & 14 & 92 & 0.80 & 0.44 & 0.11 & 0.17 \\
		\hline
		Inappropriate & 0 & 504 & 0 & 36 & 0.93 & 0 & 0 & 0 \\
		& 1 & 483 & 0 & 56 & 0.90 & 1.00 & 0.02 & 0.03 \\
		\hline
		Discriminating & 0 & 497 & 0 & 43 & 0.92 & 0 & 0 & 0 \\
		& 1 & 492 & 3 & 44 & 0.91 & 0.25 & 0.02 & 0.04 \\
		\hline
		Possibly Feedback & 0 & 531 & 0 & 9 & 0.98 & 0 & 0 & 0 \\
		& 0 & 527 & 0 & 13 & 0.98 & 0 & 0 & 0 \\
		\hline
		Personal Stories & 0 & 532 & 0 & 8 & 0.99 & 0 & 0 & 0 \\
		& 0 & 534 & 0 & 6 & 0.99 & 0 & 0 & 0 \\
		\hline
		Arguments Used & 78 & 350 & 51 & 61 & 0.79 & 0.60 & 0.56 & 0.58\\
		& 99 & 339 & 41 & 61 & 0.81 & 0.71 & 0.62 & 0.66 \\
	\end{tabular}\\
\end{frame}

\begin{frame}[fragile]
	\frametitle{Comparison: Sentiment Negative}
	\centering
	\begin{tabular}{l r r r r r}
		& Accuracy & Precision & Recall & $F_1$ \\
		\hline
		\cite{Schabus17} (best) & & 0.5842 & 0.7197 & 0.6137 \\
		\cite{Schabus17} (LSTM) & & 0.5349 & 0.7197 & 0.6137 \\
		\hline
		Our Single-Model & 0.5630 & 0.5943 & 0.5142 & 0.5513\\
		Our Multi-Model & 0.6444 & 0.6550 & 0.4571 & 0.5384 \\
	\end{tabular}\\
\end{frame}

\begin{frame}[fragile]
	\frametitle{Comparison: Sentiment Positive}
	\centering
	\begin{tabular}{l r r r r r}
		& Accuracy & Precision & Recall & $F_1$ \\
		\hline
		\cite{Schabus17} (best) & & 0.2353 & 0.4651 & 0.1333 \\
		\cite{Schabus17} (LSTM) & & 0 & 0 & 0\\
		\hline
		Our Single-Model & 0.9870 & 0 & 0 & 0 \\
		Our Multi-Model & 0.9907 & 0 & 0 & 0 \\
	\end{tabular}\\
	\vspace{5mm}
	\begin{itemize}
		\item Model learns to predict always 0 (true pos = 0, false pos = 0)
	\end{itemize}
\end{frame}

\begin{frame}[fragile]
	\frametitle{Comparison: Arguments Used}
	\centering
	\begin{tabular}{l r r r r r}
		& Accuracy & Precision & Recall & $F_1$ \\
		\hline
		\cite{Schabus17} (best) & & 0.6105 & 0.6614 & 0.6098 \\
		\cite{Schabus17} (LSTM) & & 0.5685 & 0.6458 & 0.6047\\
		\hline
		Our Single-Model & 0.7926 & 0.6047 & 0.5612 & 0.5821 \\
		Our Multi-Model & 0.8111 & 0.7071 & 0.6188 & 0.6600 \\
	\end{tabular}\\
	\vspace{5mm}
	\begin{itemize}
		\item Our Multi-Model is an improvement to the original paper
		\item Good result although category only applies 28\%
	\end{itemize}
\end{frame}


\section*{References}

\begin{frame}[t]%,allowframebreaks]
	\frametitle{References}

	\tiny%\fontsize{5}{6}\selectfont
	\bibliographystyle{apalike}
	\bibliography{references}
	\vspace{2cm}
	\small
	Code available at \url{https://github.com/oliver-pola/OneMillionPostsCorpus}
\end{frame}

\end{document}
