\documentclass[compress,aspectratio=169]{beamer} %aspectratio=169

\usetheme{Hamburg}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{lmodern}

\usepackage[english]{babel}
%\usepackage[ngerman]{babel}

\usepackage{eurosym}
\usepackage{listings}
\usepackage{lstautogobble}
\usepackage{microtype}
\usepackage{textcomp}
\usepackage{units}
\usepackage{color}
\usepackage{ulem}
\usepackage{tabularx}
\usepackage{tikz,pgfplots}
\usetikzlibrary{positioning,shapes}

\renewcommand\tabularxcolumn[1]{m{#1}}% for vertical centering text in X column

\definecolor{old-code}{gray}{0.60}

\lstset{
	basicstyle=\ttfamily\footnotesize,
	frame=single,
	numbers=left,
	language=Python,
	breaklines=true,
	breakatwhitespace=true,
	postbreak=\hbox{$\hookrightarrow$ },
	showstringspaces=false,
	autogobble=true,
	upquote=true,
	tabsize=4,
	captionpos=b,
	morekeywords={int8_t,uint8_t,int16_t,uint16_t,int32_t,uint32_t,int64_t,uint64_t,size_t,ssize_t,off_t,intptr_t,uintptr_t,mode_t}
}

\title{One Million Posts Corpus}
\subtitle{Seminar Deep Learning for Language and Speech}
\author{Jens Becker, Julius Plehn, Oliver Pola}
\institute{Language Technology Group\\Fachbereich Informatik\\Fakultät für Mathematik, Informatik und Naturwissenschaften\\Universität Hamburg}
\date{25.02.2020}

\titlegraphic{\raggedright\includegraphics[trim={0mm 0mm 1.8cm 0mm},clip,height=1.2cm]{img/logo}}

\begin{document}

\begin{frame}
	\titlepage
\end{frame}

\begin{frame}
	\frametitle{Agenda}

	\tableofcontents[hidesubsections]
\end{frame}


\section{Introduction}

\begin{frame}[fragile]
	\frametitle{Introduction}
	\begin{itemize}
		\item ... % we could just skip that, or maybe someone has sth to say?
	\end{itemize}
	\hfill\tiny\cite{Schabus17, Schabus18}
\end{frame}


\section{Corpus}

\begin{frame}[fragile]
	\frametitle{Corpus}
	\begin{itemize}
		\item One Million Posts Corpus
		\item User posts from website of Austrian daily newspaper DER STANDARD
		\item Taken over 12 months 2015-2016 
		\item 1,000,000 unlabeled posts
		\item 11,773 labeled posts
		\item Available at \url{https://ofai.github.io/million-post-corpus/}
	\end{itemize}
	\hfill\tiny\cite{Schabus17, Schabus18}
\end{frame}

\begin{frame}[fragile]
	\frametitle{Categories}
	\centering
	\begin{tabular}{l r r r r r}
		& Labeled & \multicolumn{2}{c}{Does apply} & \multicolumn{2}{c}{We apply} \\
		\hline
		Sentiment Negative & 3599 & 1691 & 47\% \\
		Sentiment Neutral & 3599 & 1865 & 52\% \\
		Sentiment Positive & 3599 & 43 & 1\% \\
		Off Topic & 3599 & 580 & 16\% \\
		Inappropriate & 3599 & 303 & 8\%\\
		Discriminating & 3599 & 282 & 8\%\\
		Possibly Feedback & 6038 & 1301 & 22\% & 72 & 2\%\\
		Personal Stories & 9336 & 1625 & 17\% & 47 & 1\%\\
		Arguments Used & 3599 & 1022 & 28\%\\
	\end{tabular}\\
	{\hfill\tiny\cite{Schabus17}}
	\vspace{5mm}
	\begin{itemize}
		\item We use only posts, that are annotated as 0 or 1 for each category
	\end{itemize}
\end{frame}


\section{Embedding}

\begin{frame}[fragile]
	\frametitle{Word2Vec Embedding}
	\begin{itemize}
		\item Using Word2Vec embedding~\cite{word2vec}
		\item Applied by Gensim~\cite{gensim}
		\item Loading pretrained german model~\cite{depset.ai}
		\item Vocabulary size = 1,309,281
		\item Embedding dim = 300
	\end{itemize}
\end{frame}

\begin{frame}[fragile]
	\frametitle{Embedding Method 1}
	\begin{itemize}
		\item Preprocess posts into lists of word indices
		\item Feed embedding matrix into training model
		\item Repeatedly feed lists of word indices into training model
		\item High memory usage in model training (GPU)
		\item Not applicable to some of our systems
	\end{itemize}
\end{frame}

\begin{frame}[fragile]
	\frametitle{Embedding Method 2}
	\begin{itemize}
		\item Preprocess posts into vectors, Gemsim can just do that
		\item No embedding matrix needed, with all the words we never see
		\item Repeatedly feed vectors into training model
		\item Lower memory usage in model training (GPU)
		\item Possible to throw away embedding model after preprocessing
		\item Even lower memory usage in general (CPU)
		\item Applicable to more systems
	\end{itemize}
\end{frame}


\section{DeepLearning}

\begin{frame}[fragile]
	\frametitle{Model}
	\begin{itemize}
		\item ...
	\end{itemize}
\end{frame}

\begin{frame}[fragile]
	\frametitle{Training}
	\begin{itemize}
		\item ...
	\end{itemize}
\end{frame}

\section{Results}

\begin{frame}[fragile]
	\frametitle{Comparison: Sentiment Negative}
	\centering
	\begin{tabular}{l r r r r r}
		& Accuracy & Precision & Recall & $F_1$ \\
		\hline
		\cite{Schabus17} (best) & & 0.5842 & 0.7197 & 0.6137 \\
		\cite{Schabus17} (LSTM) & & 0.5349 & 0.7197 & 0.6137 \\
		Our Single-Model & 0.5630 & 0.5943 & 0.5142 & 0.5513\\
		Our Multi-Model & 0.6444 & 0.6550 & 0.4571 & 0.5384 \\
	\end{tabular}\\
\end{frame}

\begin{frame}[fragile]
	\frametitle{Comparison: Sentiment Positive}
	\centering
	\begin{tabular}{l r r r r r}
		& Accuracy & Precision & Recall & $F_1$ \\
		\hline
		\cite{Schabus17} (best) & & 0.2353 & 0.4651 & 0.1333 \\
		\cite{Schabus17} (LSTM) & & 0 & 0 & 0\\
		Our Single-Model & 0.9870 & 0 & 0 & 0 \\
		Our Multi-Model & 0.9907 & 0 & 0 & 0 \\
	\end{tabular}\\
	\vspace{5mm}
	\begin{itemize}
		\item Model learns to predict always 0 (true pos = 0, false pos = 0)
	\end{itemize}
\end{frame}


\section*{References}

\begin{frame}[t]%,allowframebreaks]
	\frametitle{References}

	\tiny%\fontsize{5}{6}\selectfont
	\bibliographystyle{apalike}
	\bibliography{references}
\end{frame}

\end{document}
