%
% File report.tex
%
%% Based on the style files for ACL 2018 and NAACL 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{naaclhlt2019}
\usepackage{times}
\usepackage{latexsym}
\usepackage{todonotes}
\usepackage{url}
\usepackage{units}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{One Million Posts Corpus}

\author{Jens Becker \and Julius Plehn \and Oliver Pola \\ 
	Language Technology Group \\
	Fachbereich Informatik \\
	Fakultät für Mathematik, Informatik und Naturwissenschaften \\
	Universität Hamburg
}

\date{28.02.2020}

\begin{document}
\maketitle
\begin{abstract}
The purpose of this term paper is to summarize the development effort put into the creation of a multi-model neural network for category classification. This contains used preprocessing steps of the initial corpus, integration of an embedding layer and the actual deep learning network. Finally, an evaluation is given as well as ideas for future work. \todo{we conclude that...}

 
\end{abstract}

\section{Introduction}

...~\cite{Schabus17, Schabus18}
\todo{LSTM, multi-model, ...}

\section{Corpus}
The corpus\footnote{Corpus available at \url{https://ofai.github.io/million-post-corpus/}} used stems from the Austrian newspaper \textit{DER STANDARD} which contains posts that are labelled according to nine distinct categories. The posts have been written during a time period from 2015-2016 and contain 1 million unlabelled posts as well as 11,773 hand-labelled posts.


\subsection{Categories}
In Table~\ref{tab:categories} an overview of included categories is shown. The meaning of those categories is explained in~\cite{Schabus17}. 
In the first column is shown that for example 3599 posts were labbelled in the \textit{Off Topic} category. 
The second column shows that in this case 580 posts where indeed \textit{Off Topic}, resulting in 16\% of the labelled posts of this specific category. 

\begin{table}
	\centering\small
	\begin{tabular}{l r r r r r}
		& Labeled & \multicolumn{2}{c}{Does apply} & \multicolumn{2}{c}{We apply} \\
		\hline
		Sentiment Negative & 3599 & 1691 & 47\% \\
		Sentiment Neutral & 3599 & 1865 & 52\% \\
		Sentiment Positive & 3599 & 43 & 1\% \\
		Off Topic & 3599 & 580 & 16\% \\
		Inappropriate & 3599 & 303 & 8\%\\
		Discriminating & 3599 & 282 & 8\%\\
		Possibly Feedback & 6038 & 1301 & 22\% & 72 & 2\%\\
		Personal Stories & 9336 & 1625 & 17\% & 47 & 1\%\\
		Arguments Used & 3599 & 1022 & 28\%\\
	\end{tabular}
	\caption{Categories of posts and their distribution according to \cite{Schabus17}}
	\label{tab:categories}
\end{table}

As previously mentioned there are 11,773 labelled posts. This means after labelling 3599 posts in each category, there were another 2439 posts labeled in the \textit{Possibly Feedback} and yet another 5737 posts in the \textit{Personal Stories} category only.
These additional posts were strongly biased towards positive labels, so that the category finally applies to 22\% / 17\% of posts, whereas in a random sample like the 3599 posts it is rather 2\% / 1\%.
Besides this questionable labelling method, the additional posts can't be used in our method for technical reasons.
As we want to build a model that is capable of identifying multiple categories at once we use only posts that are annotated as 0 or 1 for each category.
So only those 3599 posts can be used in our approach.  

Looking at the percentage where the categories do apply, it becomes already clear that most of them will be problematic. 
Having only a few positive labels will encourage the model to predict always negative, as this will result in a 99\% accuracy when the positive labels are 1\% only, like in the \textit{Sentiment Positive} category. With that small set of training data it is also not applicable to omit some of the negative posts to train with an artificial balance around 50\% each.

Also there is the \textit{Off Topic} category with probably enough positives, but technically we will consider single posts only, out of their context among other posts belonging to certain articles. But the latter would be necessary to detect something as off-topic.

To summarize, we expect to produce useful results only in the \textit{Sentiment Negative}, \textit{Sentiment Neutral} and \textit{Arguments Used} categories only.

\section{Embedding}

... \todo{Describe embedding} ~\cite{word2vec} ~\cite{gensim} ~\cite{depset.ai}


\section{DeepLearning}

...


\subsection{Model}
The model has been implemented using Tensorflow 2 and Keras. Because it tends to overfit several dropouts are applied. As can be seen in Figure~\ref{fig:model} the first layer uses a dropout rate of 20\%. The following LSTM layer uses 128 units while also applying a dropout rate of 40\%. The LSTM is wrapped by a bidirectional layer which improved the performance by using information of the past, as well of the future. Finally, two dense layers are used, where the last layer maps the output of the network to the nine categories shown earlier.

\begin{figure}[h!]
	\centering
	\includegraphics[trim={1cm 19.5cm 5cm 3cm},clip,page=2, width=0.5\textwidth]{img/model}
	\caption{Applied deep learning network}
	\label{fig:model}
\end{figure}

This describes our final model, we call \textit{Multi-Model} where we use all the categories combined to a vector of size 9.
To reproduce the original work, we first implemented a classifier that was considering a single category only. 
This model we call \textit{Single-Model} and it is similar to Figure~\ref{fig:model} except having only one unit in the final classification layer.


\subsection{Training}

The training itself is supervised by using automatic learning rate adaptation (\textit{ReduceLROnPlateau}) and \textit{EarlyStopping}, which helps to reduce the overall training time.
Figure~\ref{fig:training_loss} shows the progress during training and the early stopping method applied, as training ends when validation loss starts to increase.

\begin{figure}[h!]
	\centering
	\includegraphics[trim={1.4cm 12.4cm 2cm 2.5cm},clip,width=0.5\textwidth]{img/training_All_22}
	\caption{Progress of loss function (left) and accuracy (right) during training}
	\label{fig:training_loss}
\end{figure}

The \textit{categorical accuracy} is shown in Figure~\ref{fig:training_loss} as summary over all categories. In fact it is the sum of the accuracies in each category, weighted by the rate of positive labels. Later only accuracies for single categories, extracted from a vector of predictions, are analized.

Our hyper parameters are justified by a search over a range of values, shown in Figure~\ref{fig:hyper_params}. Finally the best model measured by $F_1$ score is applied. A first impression was that out model performs well in the \textit{Arguments Used} category, so we concentrated to optimize that.

...\todo{Graph comparing different training parameters}
\begin{figure}[h!]
	\centering
	TODO
	\caption{Search for best model based on different hyper parameters}
	\label{fig:hyper_params}
\end{figure}


\section{Results}
In this paragraph the performance of the models evaluated by \newcite{Schabus17} and us are compared.
While in Table~\ref{tab:results} all measurements of the single-model as well as the multi-model of every category is shown, a few categories are highlighted.

In Table~\ref{tab:results:sentiment_negative} the performance of our model classifying the category \textit{Sentiment Negative} is shown. As the corpus for this category is nearly equally balanced this is a good starting point for a comparison. While the precision of our single- as well as the multi-model is higher, the recall is much lower that the one described in the original paper. This therefore results in a lower $F_1$ score.
\begin{table}[h!]
	\centering\tiny
	\begin{tabular}{l r r r r r}
		& Accuracy & Precision & Recall & $F_1$ \\
		\hline
		\cite{Schabus17} (best) & & 0.5842 & 0.7197 & 0.6137 \\
		\cite{Schabus17} (LSTM) & & 0.5349 & 0.7197 & 0.6137 \\
		\hline
		Our Single-Model & 0.5630 & 0.5943 & 0.5142 & 0.5513\\
		Our Multi-Model & 0.6444 & 0.6550 & 0.4571 & 0.5384 \\
	\end{tabular}  
	\caption{Comparison: Sentiment Negative}
	\label{tab:results:sentiment_negative}
\end{table}

Table~\ref{tab:results:sentiment_positive} is an example of the consequences of unbalanced data. While the accuracy is at around 99\% the precision, recall and $F_1$ score is zero. As the corpus only contains 1\% of positively labelled data for this category the model learns to predict always zero.
But the LSTM model in the original work had the exact same problem, while other models lead to non-zero but still unsatisfying scores.
\begin{table}[h!]
	\centering\tiny
	\begin{tabular}{l r r r r r}
		& Accuracy & Precision & Recall & $F_1$ \\
		\hline
		\cite{Schabus17} (best) & & 0.2353 & 0.4651 & 0.1333 \\
		\cite{Schabus17} (LSTM) & & 0 & 0 & 0\\
		\hline
		Our Single-Model & 0.9870 & 0 & 0 & 0 \\
		Our Multi-Model & 0.9907 & 0 & 0 & 0 \\
	\end{tabular}
	\caption{Comparison: Sentiment Positive}
	\label{tab:results:sentiment_positive}
\end{table}

Table~\ref{tab:results:arguments_used} shows the performance on the category \textit{Arguments Used}. In this case our multi-model LSTM outperforms the original model, as can be seen by comparing the $F_1$ score. This is especially noteworthy as this category is only applied positively in 28\% of the cases.  
\begin{table}[h!]
	\centering\tiny
	\begin{tabular}{l r r r r r}
		& Accuracy & Precision & Recall & $F_1$ \\
		\hline
		\cite{Schabus17} (best) & & 0.6105 & 0.6614 & 0.6098 \\
		\cite{Schabus17} (LSTM) & & 0.5685 & 0.6458 & 0.6047\\
		\hline
		Our Single-Model & 0.7926 & 0.6047 & 0.5612 & 0.5821 \\
		Our Multi-Model & 0.8111 & 0.7071 & 0.6188 & 0.6600 \\
	\end{tabular}
	\caption{Comparison: Arguments Used}
	\label{tab:results:arguments_used}
\end{table}


\section{Conclusion}


\bibliography{references}
\bibliographystyle{acl_natbib}
\appendix

\section{Appendices}
\label{sec:appendix}

\begin{table*}[t!]
	\centering\small
	\begin{tabular}{l r r r r r r r r}
		& True Pos & True Neg & False Pos & False Neg & Accuracy & Precision & Recall & $F_1$ \\
		\hline
		Sentiment Negative & 145 & 159 & 99 & 137 & 0.56 & 0.59 & 0.51 & 0.55 \\
		& 112 & 236 & 59 & 133 & 0.64 & 0.65 & 0.46 & 0.54 \\
		\hline
		Sentiment Neutral & 190 & 133 & 149 & 68 & 0.60 & 0.56 & 0.74 & 0.64 \\
		& 216 & 124 & 126 & 74 & 0.63 & 0.63 & 0.75 & 0.68 \\
		\hline
		Sentiment Positive & 0 & 533 & 0 & 7 & 0.99 & 0 & 0 & 0 \\
		& 0 & 535 & 0 & 5 & 0.99 & 0 & 0 & 0 \\
		\hline
		Off Topic & 0 & 452 & 0 & 88 & 0.84 & 0 & 0 & 0 \\
		& 11 & 423 & 14 & 92 & 0.80 & 0.44 & 0.11 & 0.17 \\
		\hline
		Inappropriate & 0 & 504 & 0 & 36 & 0.93 & 0 & 0 & 0 \\
		& 1 & 483 & 0 & 56 & 0.90 & 1.00 & 0.02 & 0.03 \\
		\hline
		Discriminating & 0 & 497 & 0 & 43 & 0.92 & 0 & 0 & 0 \\
		& 1 & 492 & 3 & 44 & 0.91 & 0.25 & 0.02 & 0.04 \\
		\hline
		Possibly Feedback & 0 & 531 & 0 & 9 & 0.98 & 0 & 0 & 0 \\
		& 0 & 527 & 0 & 13 & 0.98 & 0 & 0 & 0 \\
		\hline
		Personal Stories & 0 & 532 & 0 & 8 & 0.99 & 0 & 0 & 0 \\
		& 0 & 534 & 0 & 6 & 0.99 & 0 & 0 & 0 \\
		\hline
		Arguments Used & 78 & 350 & 51 & 61 & 0.79 & 0.60 & 0.56 & 0.58\\
		& 99 & 339 & 41 & 61 & 0.81 & 0.71 & 0.62 & 0.66 \\
	\end{tabular}
	\caption{Comparison of the performance of the single- (top row in each category) vs the multi-model (bottom row)}
	\label{tab:results}
\end{table*}



\end{document}
